[
{
	"uri": "/introduction/",
  "title": "Introduction",
  "section": "introduction",
	"tags": [],
	"description": "",
	"content": "Goals of the Micronaut MuShop  Explore the integration of Micronaut with cloud native services offered by Oracle Cloud Infrastructure Build and deploy microservices with Container Engine for Kubernetes (OKE) Experience Oracle Cloud services integrated within a single microservices project Provide reference implementations and sample code for real-world application development with Micronaut and Oracle Cloud  Cloud Services The MuShop application highlights several topics related to cloud native application development with Oracle Cloud Infrastructure.\n   Cloud Service Description     API Gateway Fully managed gateway for governed HTTP/S interfaces   Container Engine for Kubernetes Enterprise-grade Kubernetes on Oracle Cloud   Container Registry Highly available service to distribute container images   Email Delivery Enables sending emails   Functions Scalable, multitenant serverless functions   Monitoring Integrated metrics from all resources and services   Application Performance Monitoring Integrated distributed tracing and performance analysis   Resource Manager Infrastructure as code with Terraform   Streaming Large scale data collection and processing   Others coming soon -   Events Trigger actions in response to infrastructure changes   Notifications Broadcast messages to distributed systems   Logging Single pane of glass for resources and applications    In addition to these Cloud Native topics, MuShop demonstrates the use of several backing services available on Oracle Cloud Infrastructure.\n Autonomous Transaction Processing Database Object Storage Web Application Firewall  MuShop Services    Service Technology Cloud Services Description     src/api Micronaut  Storefront API   src/assets Node.js Object Storage Product images   src/carts Micronaut Autonomous DB (ATP) Shopping cart   src/catalogue Micronaut Autonomous DB (ATP) Product catalogue   src/dbtools Linux Autonomous DB (ATP) Database schema initializations   src/edge-router traefik  Request routing   src/events Micronaut Streaming Application event data collection   src/fulfillment Micronaut  Order processing   src/functions/newsletter-subscription Micronaut Functions Newsletter subscription   src/orders Micronaut Autonomous DB (ATP) Customer orders   src/payments Micronaut  Payment processing   src/storefront JavaScript  Store UI   src/user Micronaut Autonomous DB (ATP) Customer accounts, AuthN    "
},
{
	"uri": "/quickstart/",
  "title": "Getting Started",
  "section": "quickstart",
	"tags": ["Quickstart", "Source code"],
	"description": "",
	"content": "This project supports deployment modes for the purposes of demonstrating different functionality of Micronaut and Oracle Cloud Infrastructure.\n   Basic: deploy/basic Not supported yet Cloud Native: deploy/complete     Simplified runtime utilizing only Always Free resources deployed with Resource Manager Full-featured Kubernetes microservices deployment showcasing Oracle Cloud Native technologies and backing services    mushop └── deploy ├── basic └── complete └── docker-compose └── helm-chart └── terraform Clone Repository Each topic in this material references the source code, which should be cloned to a personal workspace.\ngit clone https://github.com/oracle-quickstart/oci-cloudnative.git mushop cd mushop git clone https://github.com/oracle-quickstart/oci-cloudnative.git dir mushop Structure The source code will look something like the following:\n#\u0026gt; mushop ├── deploy │ ├── basic │ └── complete │ ├── docker-compose │ ├── helm-chart │ └── terraform └── src ├── api ├── assets ├── carts ├── catalogue ├── edge-router ├── events ├── fulfillment ├── dbtools ├── load ├── orders ├── payment ├── storefront └── user  deploy: Collection of application deployment resources. src: Individual service source code, Dockerfiles, etc.  "
},
{
	"uri": "/micronaut/cloudnative/",
  "title": "Cloud Native Configuration",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "A significant advantage of Micronaut is the ability to use Cloud Native configuration.\nEach Micronaut application is able to define an application-oraclecloud.yml file that is automatically activated when deployed into the Oracle Cloud environment.\nThis allows the Micronaut applications to adapt to run on any Cloud and simplifies deployment.\nDecoupling distributed configuration and service discovery responsibilities leads to significant simplifications in the code.\nEach Micronaut application has no direct references to Oracle Cloud and can easily be migrated from one Cloud to another simplify by providing the appropriate configuration for the Cloud platform being used.\nAn example application-oraclecloud.yml file from the users service can be seen below (additional commentary about each configuration entry can be seen in the comments):\n# Enables export for Application metrics to Oracle Cloud Monitoring.# Note that Micronaut uses Micrometer and can support multiple metric systems simultaneously # See https://micronaut-projects.github.io/micronaut-oracle-cloud/latest/guide/#micrometermicronaut:metrics:export:oraclecloud:enabled:truenamespace:${ORACLECLOUD_METRICS_NAMESPACE:micronaut_mushop}resourceGroup:${ORACLECLOUD_METRICS_RESOURCEGROUP:user}compartmentId:${ORACLECLOUD_METRICS_COMPARTMENT_ID}# Allows Micronaut to automatically authenticate with the OCI SDK through# the use of instance prinicipals. See https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/callingservicesfrominstances.htmoci:config:instance-principal:enabled:true# Configures Micronaut to automatically generate a wallet definition and connect# to Oracle Autonomous Database using the specified OCID# See https://micronaut-projects.github.io/micronaut-oracle-cloud/latest/guide/#autonomousDatabasedatasources:default:ocid:${ORACLECLOUD_ATP_OCID}walletPassword:${ORACLECLOUD_ATP_WALLET_PASSWORD}username:${ORACLECLOUD_ATP_USERNAME}password:${ORACLECLOUD_ATP_PASSWORD}# Configures Micronaut to Export application level trace information to # Oracle Cloud Application Performance Monitoring.# See https://micronaut-projects.github.io/micronaut-oracle-cloud/latest/guide/#tracingtracing:zipkin:enabled:truesampler:probability:1http:url:${ORACLECLOUD_TRACING_ZIPKIN_HTTP_URL}path:${ORACLECLOUD_TRACING_ZIPKIN_HTTP_PATH}supportsJoin:false"
},
{
	"uri": "/micronaut/service-discovery/",
  "title": "Service Discovery",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "In a Microservice architecture services need to discover each other in a decoupled manner that is independent of the service discovery mechanism.\nMicronaut features a Service Discovery abstraction that allows service discovery to be backed onto different implementations including Kubernetes, HashiCorp Consul, Eureka and more.\nThe original MuShop application featured extensive handcrafted service discovery code in both the Spring application:\n OrdersConfigurationProperties \u0026amp; RestProxyTemplate - hand crafted Spring service discovery configuration handling OrdersConfigurationProperties \u0026amp; RestProxyTemplate - hand crafted Spring service discovery configuration handling  And Node/Express JavaScript API:\n common/index.js - Manual service discovery implementation endpoints.js - Manual service discovery configuration helpers/index.js - More manual service discovery routines  By migrating the code to Micronaut all this code could be deleted and instead encapsulated by the defining of a simple service ID and declarative client interfaces:\n@Client(id = \u0026#34;mushop-catalogue\u0026#34;, path = \u0026#34;/catalogue\u0026#34;) public interface CatalogueClient { @Get(\u0026#34;/{id}\u0026#34;) Maybe\u0026lt;Product\u0026gt; getItem(String id); } Using the service ID Micronaut provides client side lookup and load balancing and the target service to invoke is abstracted such that it can be found via explicit declaration in configuration (for example when working locally):\nmicronaut:http:services:mushop-catalogue:url:http://localhost:8082Or automatically via Micronaut\u0026rsquo;s integration with Kubernetes when deployed to a cluster.\n"
},
{
	"uri": "/micronaut/tracing/",
  "title": "Distributed Tracing",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop application featured hand crafted logic to wrap each endpoint in distributed tracing logic that can be exported to Zipkin, for example in the Go lang code which manually wraps logic in calls to opentracing.TraceServer(..):\nfunc MakeEndpoints(s Service, tracer stdopentracing.Tracer) Endpoints { return Endpoints{ ListEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /catalogue\u0026#34;)(MakeListEndpoint(s)), CountEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /catalogue/size\u0026#34;)(MakeCountEndpoint(s)), GetEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /catalogue/{id}\u0026#34;)(MakeGetEndpoint(s)), CategoriesEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /categories\u0026#34;)(MakeCategoriesEndpoint(s)), HealthEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /health\u0026#34;)(MakeHealthEndpoint(s)), } } Micronaut has built-in support for distributed tracing that only requires the addition of the micronaut-tracing module and then configuring the target Zipkin or Jaeger service to send traces to, for example in the micronaut-oraclecloud.yml configuration:\n# Configures Micronaut to Export application level trace information to # Oracle Cloud Application Performance Monitoring.# See https://micronaut-projects.github.io/micronaut-oracle-cloud/latest/guide/#tracingtracing:zipkin:enabled:truesampler:probability:1http:url:${ORACLECLOUD_TRACING_ZIPKIN_HTTP_URL}path:${ORACLECLOUD_TRACING_ZIPKIN_HTTP_PATH}supportsJoin:falseOnce enabled Micronaut will automatically publish traces received from the service and also automatically instrument any outgoing HTTP client or Kafka producers to include trace information such that trace information can be propagated from one service to another.\nThis is universally enabled across all applications without the developer having to do anything additional, whilst the original MuShop requires explicit declarations of all incoming and outgoing traced endpoints.\n"
},
{
	"uri": "/micronaut/management/",
  "title": "Health Checks &amp; Observability",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop application features extensive code to define application health observability endpoints.\nFor example the original Go lang code features manual definition of the /health endpoint in endpoints.go:\nfunc MakeHealthEndpoint(s Service) endpoint.Endpoint { return func(ctx context.Context, request interface{}) (response interface{}, err error) { health := s.Health() return healthResponse{Health: health}, nil } } As well as a manually implemented health check routine in services.go:\nfunc (s *catalogueService) Health() []Health { var health []Health dbstatus := \u0026#34;OK\u0026#34; err := s.db.Ping() if err != nil { dbstatus = \u0026#34;err\u0026#34; } app := Health{\u0026#34;catalogue\u0026#34;, \u0026#34;OK\u0026#34;, time.Now().String()} db := Health{\u0026#34;atp:catalogue-data\u0026#34;, dbstatus, time.Now().String()} health = append(health, app) health = append(health, db) return health } The existing Spring application code also features manually implemented application health logic:\n@RestController public class HealthCheckController { @ResponseStatus(HttpStatus.OK) @RequestMapping(method = RequestMethod.GET, path = \u0026#34;/health\u0026#34;) public @ResponseBody Map\u0026lt;String, List\u0026lt;HealthCheck\u0026gt;\u0026gt; getHealth() { Map\u0026lt;String, List\u0026lt;HealthCheck\u0026gt;\u0026gt; map = new HashMap\u0026lt;String, List\u0026lt;HealthCheck\u0026gt;\u0026gt;(); List\u0026lt;HealthCheck\u0026gt; healthChecks = new ArrayList\u0026lt;HealthCheck\u0026gt;(); Date dateNow = Calendar.getInstance().getTime(); HealthCheck app = new HealthCheck(\u0026#34;orders\u0026#34;, \u0026#34;OK\u0026#34;, dateNow); HealthCheck database = new HealthCheck(\u0026#34;orders-db\u0026#34;, \u0026#34;OK\u0026#34;, dateNow); healthChecks.add(app); healthChecks.add(database); map.put(\u0026#34;health\u0026#34;, healthChecks); return map; } } All of these manually crafted implementations of application health could be eliminated in the Micronaut application by adding a dependency on the micronaut-management module and adding the following definition to application.yml to expose the health information:\nendpoints:health:enabled:truesensitive:falsedetails-visible:ANONYMOUSMicronaut then exposes health readiness and liveness checks via /health/readiness and /health/liveness automatically with many built in health indicators to check that the database and messaging resources remain healthy.\nThe developer can tweak the configuration to expose the health endpoint on a different port, require authentication to access the details and more.\n"
},
{
	"uri": "/micronaut/metrics/",
  "title": "Application Metrics and Monitoring",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop applications implemented exposing metrics inconsistently with some applications exporting metrics and others not. The applications that did export metrics provided support only for Prometheus, for example in the Go code:\nr.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) Micronaut builds on Micrometer and supports exporting Metrics to over a dozen metrics backends including Prometheus and Oracle Cloud.\nEvery single application in the Micronaut MuShop demonstration exports metrics consistently and uniformally through the addition of this simple configuration:\nmicronaut:metrics:export:prometheus:enabled:truedescriptions:truestep:PT1Moraclecloud:enabled:truenamespace:${ORACLECLOUD_METRICS_NAMESPACE:micronaut_mushop}resourceGroup:${ORACLECLOUD_METRICS_RESOURCEGROUP:user}compartmentId:${ORACLECLOUD_METRICS_COMPARTMENT_ID}In addition, timers and counters can easily be added to any code in the Micronaut application simply by annotating a method with @Timed or @Counted, for example the original Helidon/Java code was written with explicit calls to add timings and meters:\npublic void deleteCartItem(ServerRequest request, ServerResponse response) { String cartId = request.path().param(\u0026#34;cartId\u0026#34;); String itemId = request.path().param(\u0026#34;itemId\u0026#34;); try { Cart cart = carts.getById(cartId); if (cart == null || !cart.removeItem(itemId)) { response.status(404).send(); return; } Timer.Context context = saveCartTimer.time(); carts.save(cart); context.close(); response.status(200).send(); } catch (Exception e) { log.log(Level.SEVERE, \u0026#34;deleteCartItem failed.\u0026#34;, e); sendError(response, e.getMessage()); return; } } This error prone logic is greatly simplified by simply adding @Timed to method in Micronaut:\n@Delete(\u0026#34;/{cartId}/items/{itemId}\u0026#34;) @Timed(\u0026#34;carts.updated.timer\u0026#34;) Cart deleteCartItem(String cartId, String itemId) { Cart cart = cartRepository.getById(cartId); if (cart == null || !cart.removeItem(itemId)) { throw new HttpStatusException(HttpStatus.NOT_FOUND, \u0026#34;Cart with id \u0026#34; + cartId + \u0026#34; not found\u0026#34;); } cartRepository.save(cart); return cart; } "
},
{
	"uri": "/micronaut/atp/",
  "title": "Connecting Autonomous Database",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop reference applications contained significant logic in order to download and configure the Oracle Wallet definition to connect to autonomous database.\nThe details of this requirement are described in Application Configuration section of the original MuShop.\nDevelopers have to enhance their CI pipeline to ensure the client credentials contained with the Oracle Wallet are packaged within the file system of the Docker container before uploading. The downside of this approach is that every time the Oracle Wallet is rotated a new container image needs to be built.\nWith Micronaut connecting to Autonomous Database is a simple matter of supplying the appropriate configuration in application-oraclecloud.yml:\n# Configures Micronaut to automatically generate a wallet definition and connect# to Oracle Autonomous Database using the specified OCID# See https://micronaut-projects.github.io/micronaut-oracle-cloud/latest/guide/#autonomousDatabasedatasources:default:ocid:${ORACLECLOUD_ATP_OCID}walletPassword:${ORACLECLOUD_ATP_WALLET_PASSWORD}username:${ORACLECLOUD_ATP_USERNAME}password:${ORACLECLOUD_ATP_PASSWORD}With this configuration in place Micronaut will use the OCI SDK to download the wallet and store the credentials in-memory, automatically configuring the underlying datasource.\n"
},
{
	"uri": "/micronaut/data/",
  "title": "Database Access",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop application includes a lot of handwritten SQL logic that is greatly simplified by the use of Micronaut Data.\nIn the Go version of the following code, SQL is appended together manually and executed in order to perform a count:\nfunc (s *catalogueService) Count(categories []string) (int, error) { query := \u0026#34;SELECT COUNT(DISTINCT products.sku) FROM products JOIN product_category ON products.sku=product_category.sku JOIN categories ON product_category.category_id=categories.category_id\u0026#34; var args []interface{} for i, t := range categories { if i == 0 { query += \u0026#34; WHERE categories.name=:categoryname\u0026#34; args = append(args, t) } else { query += \u0026#34; OR categories.name=:categoryname\u0026#34; args = append(args, t) } } sel, err := s.db.Prepare(query) if err != nil { s.logger.Log(\u0026#34;database error\u0026#34;, err) return 0, ErrDBConnection } defer sel.Close() var count int err = sel.QueryRow(args...).Scan(\u0026amp;count) if err != nil { s.logger.Log(\u0026#34;database error\u0026#34;, err) return 0, ErrDBConnection } return count, nil } The above code is littered with error prone logic to produce the correct query and ensure database connetions are closed and error handling logic correctly applied.\nThe service logic still then needs to be wired into an exposed endpoing with Go:\nfunc MakeEndpoints(s Service, tracer stdopentracing.Tracer) Endpoints { return Endpoints{ CountEndpoint: opentracing.TraceServer(tracer, \u0026#34;GET /catalogue/size\u0026#34;)(MakeCountEndpoint(s)), ... } } func MakeCountEndpoint(s Service) endpoint.Endpoint { return func(ctx context.Context, request interface{}) (response interface{}, err error) { req := request.(countRequest) n, err := s.Count(req.Categories) return countResponse{N: n, Err: err}, err } } All of this complexity is removed in the Micronaut version thanks to Micronaut Data into a simple repository declaration:\n@JdbcRepository(dialect = Dialect.ORACLE) public interface ProductRepository extends PageableRepository\u0026lt;Product, String\u0026gt; { ... int countDistinct(); int countDistinctByCategoriesNameInList(List\u0026lt;String\u0026gt; categories); } The SQL queries are automatically implemented at compilation time based on the conventions in the repository interface using the repository pattern.\nThe endpoint can then just use the repository to easily expose the necessary data over HTTP:\n@Get(\u0026#34;/catalogue/size{?categories}\u0026#34;) public CatalogueSizeDTO size(@QueryValue @Nullable List\u0026lt;String\u0026gt; categories) { if (categories == null || categories.isEmpty()) { return new CatalogueSizeDTO(productRepository.countDistinct()); } else { return new CatalogueSizeDTO(productRepository.countDistinctByCategoriesNameInList(categories)); } } The code is even more complex for listing records instead of performing a count, with Go manual pagination of records is implemented to combine with the already complex SQL concatenation logic:\nfunc (s *catalogueService) List(categories []string, order string, pageNum, pageSize int) ([]Product, error) { var products []Product query := baseQuery var args []interface{} for i, t := range categories { if i == 0 { query += \u0026#34; WHERE categories.name=:categoryname\u0026#34; args = append(args, t) } else { query += \u0026#34; OR categories.name=:categoryname\u0026#34; args = append(args, t) } } if order != \u0026#34;\u0026#34; { query += \u0026#34; ORDER BY :orderby\u0026#34; args = append(args, order) } err := s.db.Select(\u0026amp;products, query, args...) if err != nil { s.logger.Log(\u0026#34;database error\u0026#34;, err) return []Product{}, ErrDBConnection } for i, s := range products { products[i].ImageURL = []string{s.ImageURL1, s.ImageURL2} products[i].Categories = strings.Split(s.CategoryString, \u0026#34;,\u0026#34;) } products = cut(products, pageNum, pageSize) return products, nil } func cut(products []Product, pageNum, pageSize int) []Product { if pageNum == 0 || pageSize == 0 { return []Product{} // pageNum is 1-indexed \t} start := (pageNum * pageSize) - pageSize if start \u0026gt; len(products) { return []Product{} } end := (pageNum * pageSize) if end \u0026gt; len(products) { end = len(products) } return products[start:end] } All of this is unnecessary in the Micronaut version as Micronaut Data has the built in ability to bind pagination parameters from the request and perform pagination on data retrieved from the database:\n@Get(\u0026#34;/catalogue{?categories}\u0026#34;) public List\u0026lt;CatalogueItemDTO\u0026gt; list( @Nullable List\u0026lt;String\u0026gt; categories, @Nullable Pageable pageable) { Stream\u0026lt;Product\u0026gt; productStream; if (pageable == null || pageable == Pageable.UNPAGED) { productStream = productRepository.findAll().stream(); } else { productStream = productRepository.list(pageable).stream(); } return productStream.map(this::toDTO) .collect(Collectors.toList()); } private CatalogueItemDTO toDTO(Product product) { return new CatalogueItemDTO( product.getSku(), product.getBrand(), product.getTitle(), product.getDescription(), product.getWeight(), product.getProductSize(), product.getColors(), product.getQuantity(), product.getPrice(), getImageUrl(product), getCategories(product) ); } The Micronaut Data version is in fact more flexible as you can customize the default pagination parameter names and sizes through simple configuration in application.yml:\nmicronaut:data:pageable:default-page-size:-1sort-parameter-name:sortpage-parameter-name:pagesize-parameter-name:sizeFor example changing the page-parameter-sort setting allows you to change the name of the HTTP request parameter used in the query string from client requests to alter how data is sorted.\n"
},
{
	"uri": "/micronaut/openapi/",
  "title": "OpenAPI / Swagger",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "In the original MuShop sample application most of the applications defined manually crafted OpenAPI definitions that in fact were in some cases out-of-date with the actual state of the code.\nKeeping the OpenAPI documentation for a service aligned with the actual code can be a real challenge.\nWith the Micronaut MuShop every single applications defines an accessible OpenAPI definition available via Swagger UI.\nFor example you can cd into the api service and run the application with:\ncd src/api ./gradlew run The available endpoints can be browsed at localhost:8080/swagger/views/swagger-ui\nMicronaut will automatically at compilation time generate the API definition by parsing the source code and javadoc comments of the application ensuring that any javadoc and API documention exposed to the user is correctly aligned and doesn\u0026rsquo;t become stale over time.\n"
},
{
	"uri": "/micronaut/gateway/",
  "title": "Gateway Service",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop api service is written in Node/Express and services as a Gateway service to access other services.\nApart from the already mentioned manual service discovery implementation the Node Gateway service implements all routing logic manually:\napp.get(\u0026#34;/catalogue*\u0026#34;, function (req, res, next) { req.svcClient() .get(endpoints.catalogueUrl + req.url.toString()) .then(({ data }) =\u0026gt; res.json(data)) .catch(next); }); In Micronaut this is simplified through the use of service discovery combined with the declarative client to allow requests to be easily proxied:\n@MuService @Client(id = \u0026#34;mushop-catalogue\u0026#34;) // the service ID to target @Secured(SecurityRule.IS_ANONYMOUS) // allow unsecured access public interface CatalogueService { @Get(\u0026#34;/catalogue/{id}\u0026#34;) // route the request and return the response in a well defined shape  Single\u0026lt;HttpResponse\u0026lt;CatalogueItem\u0026gt;\u0026gt; getItem(String id); } The Micronaut application is also locked by down by default, meaning that any endpoint exposed should declare @Secured(SecurityRule.IS_ANONYMOUS) to explicitly allow anonymous access and avoid serious security vulnerabilities emerging.\n"
},
{
	"uri": "/micronaut/security/",
  "title": "Security",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original MuShop api service includes a lot of error prone logic to assert security rules at the API gateway layer, for example:\nrouter.get(\u0026#39;/orders\u0026#39;, (req, res) =\u0026gt; { if (!helpers.isLoggedIn(req)) { return next(helpers.createError(\u0026#34;User not logged in.\u0026#34;, 401)); } const custId = helpers.getCustomerId(req); res.json(this.orders.find(row =\u0026gt; custId === row.customer.id)); }) It is very easy to forget an isLoggedIn call causing a security vulnerability in your application.\nThe Micronaut version of the Gateway includes the Micronaut Security module which is locked down by default with every request to the API service by default returning a 401 unless the user is logged in.\nServices that need to allow anonymous access are declared as such through the presence of @Secured(SecurityRule.IS_ANONYMOUS):\n@MuService @Client(id = ServiceLocator.CATALOGUE) @Secured(SecurityRule.IS_ANONYMOUS) public interface CatalogueService { ... } "
},
{
	"uri": "/quickstart/dockecompose/",
  "title": "[Local] Docker compose",
  "section": "quickstart",
	"tags": ["Docker", "Docker Compose"],
	"description": "",
	"content": "The complete Micronaut MuShop application can be run using docker-compose locally.\nDefault Configuration The default configuration that the docker-compose file uses is to be offline.\nWhen using docker-compose some services are not available, such as publishing the newsletter which uses an Oracle Cloud Function, however the remaining services are functional.\nPrerequisites You need Docker and Docker Compose installed locally and at least 8GB of resources assigned to Docker in order to run instances of Oracle database within containers.\nQuick Start Pull images Start by pulling all necessary images. Note that it may take a little while to download all the images (in particular the Oracle Database image is 2GB to download).\ncd deploy/complete/docker-compose docker-compose pull dir deploy/complete/docker-compose docker-compose pull Run stack After the images are downloaded start the stack with the detached state -d option.\ndocker-compose up -d Note that it may take a little while to spin up the containers.\n You can track the progress of services starting up with docker-compose ps and once all services have a status of Up then open http://localhost:81 in your browser to access MuShop.\nShutdown docker-compose down "
},
{
	"uri": "/quickstart/terraform/",
  "title": "[Oracle Cloud] Terraform + Helm",
  "section": "quickstart",
	"tags": ["Terraform", "OKE", "Helm", "Setup"],
	"description": "",
	"content": "The terraform configuration scripts can be used to completely install MuShop to Oracle Cloud along with necessary Oracle Cloud Infrastructure services.\nPrerequisites You need to install terraform locally.\nConfiguration Create copy of terraform variables file terraform.tfvars.example.\ncp terraform.tfvars.example mushop.tfvars Then edit the mushop.tfvars:\n  Configure the OCI authentication and deployment variables. Follow the guidelines on how to obtain authentication details and configure the provider.:\n# OCI authenticationtenancy_ocid=\u0026#34;ocid1.tenancy.....\u0026#34;fingerprint=\u0026#34;\u0026#34;# e.g.: \u0026#34;5f:53:...\u0026#34; or leave blank if using CloudShelluser_ocid=\u0026#34;\u0026#34;# e.g.: \u0026#34;ocid1.user...\u0026#34; or leave blank if using CloudShellprivate_key_path=\u0026#34;\u0026#34;# e.g.: \u0026#34;/users/user/.oci/oci_api_key.pem\u0026#34; or leave blank if using CloudShell# Deployment compartmentcompartment_ocid=\u0026#34;ocid1.compartment....\u0026#34;# regionregion=\u0026#34;us-ashburn-1\u0026#34;   Enable streaming service (optional):\n# Streamingcreate_oracle_streaming_service_stream=trueNote that by configuring this option the terraform will configure Oracle Cloud Streaming service. The default quota allows the account to have 1 stream. In case you\u0026rsquo;re out of the quota keep this option disabled, the MuShop application won\u0026rsquo;t be affected.\n   Enable newsletter service (optional):\n# Newslettercreate_oracle_function_newsletter=truenewsletter_function_approved_email_address=\u0026#34;micronaut-newsletter@mushop.com\u0026#34;Note that by configuring this option the terraform will configure Oracle Functions, Oracle API Gateway and Oracle Email Delivery Service. The default quota allows the account to have 1 API Gateway configured. In case you\u0026rsquo;re out of the quota keep this option disabled, the MuShop application won\u0026rsquo;t be affected.\n   Enable Application Performance Monitoring: The Oracle Cloud Application Performance Monitoring service doesn\u0026rsquo;t provide the Terraform provider so it needs to be created manually before running the terraform:\n Navigate to Observability \u0026amp; Management -\u0026gt; Application Performance Monitoring -\u0026gt; Administration. Hit the button Create APM domain and enter the domain name. Once created make a note of Data Upload Endpoint and auto_generated_public_key. Edit the mushop.tfvars:  # Tracingapm_zipkin_enabled=falseapm_zipkin_url=\u0026#34;\u0026#34;# Copy here the APM domain Data Upload Endpoint.apm_zipkin_path=\u0026#34;\u0026#34;# The format is: /20200101/observations/public-span?dataFormat=zipkin\u0026amp;dataFormatVersion=2\u0026amp;dataKey=\u0026lt;auto_generated_public_key\u0026gt;  Installation From the directory deploy/complete/terraform:\n  Init the providers:\nterraform init   Plan the execution (Optional):\nterraform plan --var-file mushop.tfvars   Execute the plan:\nterraform apply --var-file mushop.tfvars Note the deployment of all resources may take a while (~20 minutes).\n After successful deployment you should see output variables like below:\nOutputs: autonomous_database_password = \u0026quot;....\u0026quot; comments = \u0026quot;The application URL will be unavailable for a few minutes after provisioning while the application is configured and deployed to Kubernetes\u0026quot; deploy_id = \u0026quot;R8Mm\u0026quot; deployed_oke_kubernetes_version = \u0026quot;v1.19.7\u0026quot; deployed_to_region = \u0026quot;us-ashburn-1\u0026quot; dev = \u0026quot;Made with ❤ by Oracle Developers\u0026quot; external_ip = \u0026quot;129.159.91.46\u0026quot; generated_private_key_pem = \u0026lt;\u0026lt;EOT ... -----END RSA PRIVATE KEY----- EOT grafana_admin_password = \u0026quot;.....\u0026quot; grafana_url = \u0026quot;http://129.159.91.46/grafana\u0026quot; kubeconfig_for_kubectl = \u0026quot;export KUBECONFIG=./generated/kubeconfig\u0026quot; mushop_source_code = \u0026quot;https://github.com/pgressa/oraclecloud-cloudnative\u0026quot; mushop_url = \u0026quot;http://129.159.91.46\u0026quot; mushop_url_button = \u0026quot;http://129.159.91.46\u0026quot; mushop_url_https = \u0026quot;https://129.159.91.46\u0026quot; To access MuShop use the mushop_url_https output variable.\n  Cleanup From the directory deploy/complete/terraform destroy the stack:\n# From this directory terraform destroy "
},
{
	"uri": "/micronaut/messaging/",
  "title": "Messaging",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "The original Spring-based application includes extensive logic to send NATs messages.\nThis is greatly simplified in Micronaut with the Micronaut NATS module:\n@NatsClient public interface OrdersPublisher { @Subject(\u0026#34;mushop-orders\u0026#34;) void dispatchToFulfillment(OrderUpdate orderUpdate); } In the above example Micronaut automatically implements the logic to send a message and handle any errors, avoiding 115 lines of additional code.\nTo Go application that sends events via OCI streaming again features over a 100 lines of code to send messages with the OCI SDK. This logic is coupled to OCI and not portable across Clouds.\nThe Micronaut version is simplified through the use of the Micronaut Kafka module which encapsulates the logic needed to define Kafka producers.\n@KafkaClient(batch = true) public interface EventProducer { @Topic(\u0026#34;events\u0026#34;) void send(EventRecord... eventRecords); } Since the Kafka API is used to send the message internally, the application can be moved to any Cloud infrastructure that features a Kafka compatible API.\n"
},
{
	"uri": "/micronaut/httpclient/",
  "title": "HTTP Client",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "Micronaut features an HTTP client that is included by default and automatically instrumented for distributed tracing, service discovery and metrics. The Spring application in the original MuShop features many manually written HTTP calls that amount to another 100 lines of additional code.\nThis HTTP client logic was completely removed in the Micronaut application as the existing HTTP client was simply injected via service discovery:\npublic OrdersService(CustomerOrderRepository customerOrderRepository, MeterRegistry meterRegistry, OrdersPublisher ordersPublisher, PaymentClient paymentClient, OrdersConfiguration ordersConfiguration, @Client(\u0026#34;users\u0026#34;) RxHttpClient userClient, // HTTP clients injected here  @Client(\u0026#34;carts\u0026#34;) RxHttpClient cartsClient) { ... } "
},
{
	"uri": "/micronaut/deployment/",
  "title": "Deployment",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "Each example application in the original MuShop features a hand-crafted Dockerfile that the developer has to write and maintain including ensuring all images used in the Dockerfile are kept up-to-date to avoid security vulnerabilities.\nNone of the Micronaut applications feature a Dockerfile which is provided automatically and maintained by the framework across releases, making deploying an image a simple matter of executing:\n./gradlew dockerPush for Gradle, or alternatively for Maven:\n./mvnw deploy -Dpackaging=docker Deploying a native version of the application with GraalVM is equally simple:\n./gradlew dockerPushNative for Gradle, or alternatively for Maven:\n./mvnw deploy -Dpackaging=docker-native "
},
{
	"uri": "/micronaut/native/",
  "title": "GraalVM Native Image",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "Every single application in the Micronaut MuShop can be built into a GraalVM native image and deployed natively as well as in JIT mode allowing the developer to flexibly choose their preferred deployment model and benefit from massive reductions in memory usage and startup time when choosing native.\nTo build any of the services into a native image simply navigate to the to the project directory and if using Gradle run:\n./gradlew nativeImage Or if the project is using Maven run:\n./mvnw package -Dpackaging=native-image "
},
{
	"uri": "/quickstart/helm/",
  "title": "[Oracle Cloud] Helm",
  "section": "quickstart",
	"tags": ["Kubernetes", "OKE", "Setup", "Helm"],
	"description": "",
	"content": "This deployment option utilizes helm for configuration and installation onto a Kubernetes cluster. It is recommended to use an Oracle Container Engine for Kubernetes cluster, however other standard Kubernetes clusters will also work.\ncd deploy/complete/helm-chart dir deploy/complete/helm-chart  Path for Cloud Native deployment configurations using helm\n Deploying the complete MuShop application with backing services from Oracle Cloud Infrastructure involves the use of the following helm charts:\n setup: Installs umbrella chart dependencies on the cluster (optional) Not supported yet provision: Provisions OCI resources integrated with Service Broker (optional) mushop: Deploys the MuShop application runtime  To get started, create a namespace for the application and its associative deployments:\nkubectl create ns mushop  Setup MuShop provides an umbrella helm chart called setup, which includes several recommended installations on the cluster. These represent common 3rd party services, which integrate with Oracle Cloud Infrastructure or enable certain application features.\n   Chart Purpose Option Default     Prometheus Service metrics aggregation prometheus.enabled true   Grafana Infrastructure/service visualization dashboards grafana.enabled true   Metrics Server Support for Horizontal Pod Autoscaling metrics-server.enabled true   Ingress Nginx Ingress controller and public Load Balancer ingress-nginx.enabled true   Service Catalog Service Catalog chart utilized by Oracle Service Broker catalog.enabled true   Cert Manager x509 certificate management for Kubernetes cert-manager.enabled true   Jenkins Jenkins automation server on Kubernetes jenkins.enabled false     Dependencies installed with setup chart. NOTE as these are very common installations, each may be disabled as needed to resolve conflicts.\n From deploy/complete/helm-chart directory:\n  Install chart dependencies:\nhelm dependency update setup   Install setup chart:\nhelm install setup \\ --name mushop-utils \\ --namespace mushop-utilities kubectl create ns mushop-utilities helm install mushop-utils setup \\ --namespace mushop-utilities    OPTIONAL The Jenkins automation server can be enabled by setting jenkins.enabled to true in values.yaml or by adding the command line flag --set jenkins.enabled=true in the helm install command above.\n helm install mushop-utils setup \\ --namespace mushop-utilities \\ --set jenkins.enabled=true   NOTE the public EXTERNAL-IP assigned to the ingress controller load balancer:\nkubectl get svc mushop-utils-ingress-nginx-controller \\  --namespace mushop-utilities   Deploy MuShop Deploying the full application requires cloud backing services from Oracle Cloud Infrastructure. These services must be provisioned manually and are configured using kubernetes secrets.\nConfigure   Provision an Autonomous Transaction Processing (ATP) database. Once RUNNING download the DB Connection Wallet and configure secrets as follows:\n  Create oadb-admin secret containing the database administrator password. Used once for schema initializations.\nkubectl create secret generic oadb-admin \\  --namespace mushop \\  --from-literal=oadb_admin_pw=\u0026#39;\u0026lt;DB_ADMIN_PASSWORD\u0026gt;\u0026#39;   Create oadb-wallet secret with the Wallet contents using the downloaded Wallet_*.zip. The extracted Wallet_* directory is specified as the secret contents.\nkubectl create secret generic oadb-wallet \\  --namespace mushop \\  --from-file=\u0026lt;PATH_TO_EXTRACTED_WALLET_FOLDER\u0026gt;   Create oadb-connection secret with the Wallet password and the service TNS name to use for connections.\nkubectl create secret generic oadb-connection \\  --namespace mushop \\  --from-literal=oadb_wallet_pw=\u0026#39;\u0026lt;DB_WALLET_PASSWORD\u0026gt;\u0026#39; \\  --from-literal=oadb_service=\u0026#39;\u0026lt;DB_TNS_NAME\u0026gt;\u0026#39; \\  --from-literal=oadb_ocid=\u0026#39;\u0026lt;DB_OCID\u0026gt;\u0026#39; \\  Each database has 5 unique TNS Names displayed when the Wallet is downloaded an example would be mushopdb_TP.\n     Optional: Instead of creating a shared database for the entire application, you may establish full separation of services by provisioning individual ATP instances for each service that requires a database. To do so, repeat the previous steps for each database,and give each secret a unique name, for example: carts-oadb-admin, carts-oadb-connection, carts-oadb-wallet.\n carts catalogue orders user    Provision a Streaming instance from the Oracle Cloud Infrastructure Console, and make note of the created Stream Pool configuration values bootstrapServers and stream pool ID.\n  Create oss-connection secret containing the Stream connection details.\nkubectl create secret generic oss-connection \\  --namespace mushop \\  --from-literal=bootstrapServers=\u0026#39;\u0026lt;OSS STREAM BOOTSTRAP SERVERS\u0026gt;\u0026#39; \\  --from-literal=jaasConfig=\u0026#39;\u0026lt;JAAS CONFIG\u0026gt;\u0026#39;   Note that \u0026lt;OSS STREAM BOOTSTRAP SERVERS\u0026gt; and \u0026lt;JAAS CONFIG\u0026gt; values can can be found in the Stream Pool -\u0026gt; Kafka Connection Setting. In case you want to connect under different user then the \u0026lt;JAAS CONFIG\u0026gt; format is:\njaasConfig=\u0026quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026quot;\u0026lt;USER_COMPARTMENT_NAME\u0026gt;/\u0026lt;USER_NAME\u0026gt;/\u0026lt;OSS_POOL_ID\u0026gt;\\\u0026quot; password=\\\u0026quot;\u0026lt;USER_TOKEN\u0026gt;\\\u0026quot;;\u0026quot; Make sure the user has permission to write to the given stream.\n  Configure a config map with deployment details:\nkubectl create cm oci-deployment \\  --namespace mushop \\  --from-literal=compartment_id=\u0026#39;\u0026lt;COMPARTMENT ID\u0026gt;\u0026#39; \\  --from-literal=region=\u0026#39;\u0026lt;OCI REGION\u0026gt;\u0026#39;   Optional: If you want to configure and use Oracle Application Monitoring service, make sure you create the APM domain and create K8s secret with connection details.\n Navigate to Observability \u0026amp; Management -\u0026gt; Application Performance Monitoring -\u0026gt; Administration. Hit the button Create APM domain and enter the domain name. Once created make a note of Data Upload Endpoint and auto_generated_public_key. Edit the mushop.tfvars:  kubectl create secret generic oss-connection \\  --namespace mushop \\  --from-literal=zipkin_enabled=true \\  --from-literal=zipkin_path=\u0026#39;\u0026lt;APM DOMAIN DATA UPLOAD ENDPOINT\u0026gt;\u0026#39; \\  --from-literal=zipkin_url=\u0026#39;/20200101/observations/public-span?dataFormat=zipkin\u0026amp;dataFormatVersion=2\u0026amp;dataKey=\u0026lt;AUTO_GENERATED_PUBLIC_KEY\u0026gt;\u0026#39;   Optional: If you want to configure and use the functions/API Gateway functionality, make sure you create and deploy the function and the API Gateway by following the instructions in the src/functions/newsletter-subscription folder.\nTo configure the api chart to use the newsletter subscribe function, create K8s external service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - piVersion: v1 kind: Service metadata: name: mushop-newsletter namespace: mushop spec: externalName: \u0026lt;API_GATEWAY_URL\u0026gt; ports: - port: 443 protocol: TCP targetPort: 443 type: ExternalName status: loadBalancer: {} EOF Replace the API_GATEWAY_URL with an actual hostname URL you got when you deployed your API gateway instance. For example: jbwyanwkmxqweq.apigateway.us-ashburn-1.oci.customer-oci.com\n  Make a copy of the values-dev.yaml file in this directory. Then complete the missing values (e.g. secrets) like the following:\nglobal:ossConnectionSecret:oss-connection# Name of Stream connection secretoadbAdminSecret:oadb-admin# Name of DB Admin secretoadbWalletSecret:oadb-wallet# Name of Wallet secretoadbConnectionSecret:oadb-connection# Name of DB Connection secretociDeploymentConfigMap:oci-deployment# Name of Deployment details config maptags:atp:true# General flag to use Oracle Autonomous Databasestreaming:true# General flag to use Oracle Streaming Service NOTE: If it\u0026rsquo;s desired to connect a separate databases for a given service, you can specify values specific for each service, such as carts.oadbAdminSecret, carts.oadbWalletSecret\u0026hellip;\n   Deploy   From deploy/complete/helm-chart directory:\nhelm install mushop mushop \\  --namespace mushop \\  -f \u0026lt;edited values-dev.yaml\u0026gt;   Wait for services to be Ready:\nkubectl get pod --watch --namespace mushop   Open a browser with the EXTERNAL-IP created during setup, OR port-forward directly to the edge service resource:\nkubectl port-forward \\  --namespace mushop \\  svc/edge 8000:80  Using port-forward connecting localhost:8000 to the edge service\n kubectl get svc mushop-utils-ingress-nginx-controller \\  --namespace mushop-utilities  Locating EXTERNAL-IP for Ingress Controller. NOTE this will be localhost on local clusters.\n   It may take a few moments to download all the application images. It is also normal for some pods to show errors in mock mode.  Cleanup The following list represents cleanup operations, which may vary depending on the actions performed for setup and deployment of MuShop.\n  List any helm releases that may have been installed:\nhelm list --all-namespaces NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mushop mushop 1 2020-01-31 21:14:48.511917 -0600 CST deployed mushop-0.1.0 1.0 oci-broker mushop-utilities 1 2020-01-31 20:46:30.565257 -0600 CST deployed oci-service-broker-1.3.3 mushop-provision mushop 1 2020-01-31 21:01:54.086599 -0600 CST deployed mushop-provision-0.1.0 0.1.0 mushop-utils mushop-utilities 1 2020-01-31 20:32:05.864769 -0600 CST deployed mushop-setup-0.0.1 1.0   Remove the application from Kubernetes where --name mushop was used during install:\nhelm delete mushop -n mushop   If used OCI Service broker, remove the provision dependency installation, including ATP Bindings (Wallet, password) and instances:\nhelm delete mushop-provision -n mushop After delete, kubectl get serviceinstances -A will show resources that are deprovisioning\n   If used OCI Service broker, remove the oci-broker installation:\nhelm delete oci-broker -n mushop-utilities   Uninstall Istio service mesh (if applicable):\nistioctl manifest generate --set profile=demo | kubectl delete -f -   Remove the setup cluster dependency installation:\nhelm delete mushop-utils -n mushop-utilities   "
},
{
	"uri": "/micronaut/",
  "title": "Micronaut Advantages",
  "section": "micronaut",
	"tags": [],
	"description": "",
	"content": "This section of the documentation covers the advantages that using Micronaut over the existing technologies used in the original MuShop reference application.\n"
},
{
	"uri": "/tags/docker/",
  "title": "Docker",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/docker-compose/",
  "title": "Docker Compose",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/helm/",
  "title": "Helm",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/oke/",
  "title": "OKE",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/setup/",
  "title": "Setup",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
  "title": "Tags",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/terraform/",
  "title": "Terraform",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kubernetes/",
  "title": "Kubernetes",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/quickstart/",
  "title": "Quickstart",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/source-code/",
  "title": "Source code",
  "section": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/resources/",
  "title": "More Resources",
  "section": "",
	"tags": [],
	"description": "",
	"content": "Oracle Cloud Infrastructure  Sign Up Console REST APIs SDKs CLI Developer Tools Terraform Provider Blogs Stack Overflow  OCI Service Broker  GitHub Repo Installation Services  Object Storage Service Autonomous Transaction Processing Autonomous Data Warehouse Streaming service      Tab one content const foo: string = \u0026#39;blah blah\u0026#39;;  Tab two content Tab three content    "
},
{
	"uri": "/",
  "title": "Cloud Native on Oracle Cloud Infrastructure",
  "section": "",
	"tags": [],
	"description": "",
	"content": "MuShop MuShop is a microservices demo application purpose-built to showcase interoperable Cloud Native services on Oracle Cloud Infrastructure, and to demonstrate a number of cloud native methodologies.\nThe premise of MuShop is an e-commerce website offering a variety of cat products. It represents a polyglot microservice application, with actual use case scenarios for many Oracle Cloud Infrastructure services.\nEach microservice in MuShop is designed to highlight its own, or common high-level subject in Oracle Cloud Infrastructure, using the context of the overall demo application.\n        Microservices \nμ \nMu \nMeow \n   "
},
{
	"uri": "/acknowledgements/",
  "title": "Acknowledgements",
  "section": "",
	"tags": [],
	"description": "",
	"content": "Community This project, and its associative materials utilizes many excellent projects from the open source community. While there are numerous, the following projects deserve some special recognition:\n Sock Shop: Comprehensive microservices demo from Weaveworks Hugo: Documentation tool used to create this documentation Reveal.js: HTML presentation tool used to develop workshop materials Roman Chekurov: Inspiration and basis for MuShop UI  Contributors Thanks to all our contributors for their commitment to open source!\n @junior 1247 contributions     @mvandervliet 574 contributions     @jeevanjoseph 437 contributions     @peterj 147 contributions     @juliocamara 50 contributions     @naikvenu 35 contributions     @dependabot[bot] 31 contributions     @kd7edg 15 contributions     @jjspiegel 14 contributions     @gvenzl 2 contributions     @sumikr 2 contributions     @benofben 1 contributions     @oegentil 1 contributions     @allgao 1 contributions     @rml1997 1 contributions     "
},
{
	"uri": "/categories/",
  "title": "Categories",
  "section": "categories",
	"tags": [],
	"description": "",
	"content": ""
}]